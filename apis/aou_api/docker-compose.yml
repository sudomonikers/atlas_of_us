
services:
  # Embeddings server using llama-cpp
  embeddings:
    image: ghcr.io/ggerganov/llama.cpp:server
    container_name: aou_embeddings
    command: >
      --hf-repo sudomoniker/all-MiniLM-L6-v2-Q8_0-GGUF
      --embeddings
      --host 0.0.0.0
      --port 8080
    # No external ports - internal only
    volumes:
      - embeddings_cache:/root/.cache
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      timeout: 5s
      retries: 3
      start_period: 1s

  # Atlas of Us API
  api:
    build: .
    container_name: aou_api
    ports:
      - "8080:8080"
    env_file:
      - .env
    environment:
      - EMBEDDING_ENDPOINT=http://embeddings:8080/embedding
    depends_on:
      embeddings:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8001/api/"]
      timeout: 5s
      retries: 3
      start_period: 1s

volumes:
  embeddings_cache: